{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='ticks',\n",
    "        color_codes=True, rc={'legend.frameon': False})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 19 10:15:50 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro GV100        Off  | 00000000:37:00.0 Off |                  Off |\n",
      "| 32%   44C    P2    36W / 250W |   3011MiB / 32508MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0       452      G   paraview                                     167MiB |\n",
      "|    0      5135      G   ...t/nrel/apps/paraview/5.6.0/lib/paraview   178MiB |\n",
      "|    0      5869      C   /home/jvermaas/lib/vmd/vmd_LINUXAMD64        311MiB |\n",
      "|    0      8098      G   ...t/nrel/apps/paraview/5.6.0/lib/paraview    87MiB |\n",
      "|    0      9462      G   paraview                                     204MiB |\n",
      "|    0     11716      G   paraview                                     115MiB |\n",
      "|    0     14128      G   ...t/nrel/apps/paraview/5.6.0/lib/paraview   117MiB |\n",
      "|    0     19337      G   /usr/bin/X                                   323MiB |\n",
      "|    0     27700      C   ...olecule/pstjohn/envs/tf2_gpu/bin/python  1127MiB |\n",
      "|    0     28040      G   paraview                                     136MiB |\n",
      "|    0     34241      G   ...t/nrel/apps/paraview/5.6.0/lib/paraview   117MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "WARNING: infoROM is corrupted at gpu 0000:37:00.0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pstjohn/Research/20200608_redox_calculations/spin_gnn\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import nfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from preprocess_inputs import preprocessor\n",
    "preprocessor.from_json('tfrecords/preprocessor.json')\n",
    "\n",
    "from loss import AtomInfMask, KLWithLogits\n",
    "\n",
    "def parse_example(example):\n",
    "    parsed = tf.io.parse_single_example(example, features={\n",
    "        **preprocessor.tfrecord_features,\n",
    "        **{'spin': tf.io.FixedLenFeature([], dtype=tf.string)}})\n",
    "\n",
    "    # All of the array preprocessor features are serialized integer arrays\n",
    "    for key, val in preprocessor.tfrecord_features.items():\n",
    "        if val.dtype == tf.string:\n",
    "            parsed[key] = tf.io.parse_tensor(\n",
    "                parsed[key], out_type=preprocessor.output_types[key])\n",
    "    \n",
    "    # Pop out the prediction target from the stored dictionary as a seperate input\n",
    "    parsed['spin'] = tf.io.parse_tensor(parsed['spin'], out_type=tf.float64)\n",
    "    \n",
    "    spin = parsed.pop('spin')\n",
    "    \n",
    "    return parsed, spin\n",
    "\n",
    "max_atoms = 80\n",
    "max_bonds = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Here, we have to add the prediction target padding onto the input padding\n",
    "padded_shapes = (preprocessor.padded_shapes(max_atoms=None, max_bonds=None), [None])\n",
    "\n",
    "padding_values = (preprocessor.padding_values,\n",
    "                  tf.constant(np.nan, dtype=tf.float64))\n",
    "\n",
    "num_train = len(np.load('split.npz', allow_pickle=True)['train'])\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset('tfrecords/train.tfrecord.gz', compression_type='GZIP')\\\n",
    "    .map(parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .cache().shuffle(buffer_size=num_train).repeat()\\\n",
    "    .padded_batch(batch_size=batch_size,\n",
    "                  padded_shapes=padded_shapes,\n",
    "                  padding_values=padding_values)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDense(layers.Layer):\n",
    "    \"\"\" Layer to combine the concatenation and two dense layers \"\"\"\n",
    "    def build(self, input_shape):\n",
    "        num_features = input_shape[0][-1]\n",
    "        self.concat = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(2 * num_features, activation='relu')\n",
    "        self.dense2 = layers.Dense(num_features)        \n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        \n",
    "        output = self.concat(inputs)\n",
    "        output = self.dense1(output)\n",
    "        output = self.dense2(output)\n",
    "        return output\n",
    "\n",
    "class GraphLayer(layers.Layer):\n",
    "    \"\"\" Base class for all GNN layers \"\"\"\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) == 4:\n",
    "            self.use_global = True\n",
    "            self.tile = Tile()\n",
    "            \n",
    "        elif len(input_shape) == 3:\n",
    "            self.use_global = False\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError(\"wrong input shape\")        \n",
    "        \n",
    "    \n",
    "class EdgeUpdate(GraphLayer):\n",
    "    def build(self, input_shape):\n",
    "        \"\"\" inputs = [atom_state, bond_state, connectivity]\n",
    "        shape(bond_state) = [batch, num_bonds, bond_features]\n",
    "        \"\"\"\n",
    "        super(EdgeUpdate, self).build(input_shape)\n",
    "        \n",
    "        bond_features = input_shape[1][-1]\n",
    "        \n",
    "        self.gather = nfp.Gather()\n",
    "        self.slice1 = nfp.Slice(np.s_[:, :, 1])\n",
    "        self.slice0 = nfp.Slice(np.s_[:, :, 0])\n",
    "        \n",
    "        self.concat = ConcatDense()\n",
    "        self.add = layers.Add()\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\" Inputs: [atom_state, bond_state, connectivity]\n",
    "            Outputs: bond_state\n",
    "        \"\"\"\n",
    "        if not self.use_global:\n",
    "            atom_state, bond_state, connectivity = inputs\n",
    "        else:\n",
    "            atom_state, bond_state, connectivity, global_state = inputs\n",
    "            global_state = self.tile([global_state, bond_state])\n",
    "            \n",
    "        # Get nodes at start and end of edge\n",
    "        source_atom = self.gather([atom_state, self.slice1(connectivity)])\n",
    "        target_atom = self.gather([atom_state, self.slice0(connectivity)])\n",
    "\n",
    "        if not self.use_global:\n",
    "            new_bond_state = self.concat([bond_state, source_atom, target_atom])\n",
    "        else:\n",
    "            new_bond_state = self.concat([bond_state, source_atom, target_atom, global_state])\n",
    "            \n",
    "        new_bond_state = self.add([bond_state, new_bond_state])        \n",
    "        return new_bond_state\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[1]\n",
    "    \n",
    "    \n",
    "class NodeUpdate(GraphLayer):\n",
    "    def build(self, input_shape):\n",
    "        super(NodeUpdate, self).build(input_shape)\n",
    "        \n",
    "        num_features = input_shape[1][-1]\n",
    "        \n",
    "        self.gather = nfp.Gather()\n",
    "        self.slice0 = nfp.Slice(np.s_[:, :, 0])        \n",
    "        self.slice1 = nfp.Slice(np.s_[:, :, 1])\n",
    "\n",
    "        self.concat = ConcatDense()\n",
    "        self.reduce = nfp.Reduce(reduction='sum')\n",
    "        \n",
    "        self.dense1 = layers.Dense(2 * num_features, activation='relu')\n",
    "        self.dense2 = layers.Dense(num_features)            \n",
    "        self.add = layers.Add()\n",
    "            \n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\" Inputs: [atom_state, bond_state, connectivity]\n",
    "            Outputs: atom_state\n",
    "        \"\"\"\n",
    "        if not self.use_global:\n",
    "            atom_state, bond_state, connectivity = inputs\n",
    "        else:\n",
    "            atom_state, bond_state, connectivity, global_state = inputs\n",
    "            global_state = self.tile([global_state, bond_state])\n",
    "                    \n",
    "        source_atom = self.gather([atom_state, self.slice1(connectivity)])\n",
    "        \n",
    "        if not self.use_global:\n",
    "            messages = self.concat([source_atom, bond_state])\n",
    "        else:\n",
    "            messages = self.concat([source_atom, bond_state, global_state])\n",
    "            \n",
    "        new_atom_state = self.reduce([messages, self.slice0(connectivity), atom_state])\n",
    "        \n",
    "        # Dense net after message reduction\n",
    "        new_atom_state = self.dense1(new_atom_state)\n",
    "        new_atom_state = self.dense2(new_atom_state)\n",
    "        new_atom_state = self.add([atom_state, new_atom_state])\n",
    "        \n",
    "        return new_atom_state\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "class Tile(layers.Layer):    \n",
    "    def call(self, inputs):\n",
    "        global_state, target = inputs\n",
    "        target_shape = tf.shape(target)[1]  # number of edges or nodes\n",
    "        expanded = tf.expand_dims(global_state, 1)\n",
    "        return tf.tile(expanded, tf.stack([1, target_shape, 1]))\n",
    "\n",
    "\n",
    "class GlobalUpdate(GraphLayer):\n",
    "    def __init__(self, units, num_heads, **kwargs):\n",
    "        super(GlobalUpdate, self).__init__(**kwargs)\n",
    "        self.units = units          # H\n",
    "        self.num_heads = num_heads  # N\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(GlobalUpdate, self).build(input_shape)        \n",
    "        dense_units = self.units * self.num_heads  # N*H\n",
    "        self.query_layer = layers.Dense(self.num_heads, name='query')\n",
    "        self.value_layer = layers.Dense(dense_units, name='value')\n",
    "        self.add = layers.Add()\n",
    "        \n",
    "    def transpose_scores(self, input_tensor):\n",
    "        input_shape  = tf.shape(input_tensor)\n",
    "        output_shape = [input_shape[0], input_shape[1], self.num_heads, self.units]\n",
    "        output_tensor = tf.reshape(input_tensor, output_shape)\n",
    "        return tf.transpose(a=output_tensor, perm=[0, 2, 1, 3])  # [B,N,S,H]\n",
    "       \n",
    "    def call(self, inputs, mask=None):\n",
    "        \n",
    "        if not self.use_global:\n",
    "            atom_state, bond_state, connectivity = inputs\n",
    "        else:\n",
    "            atom_state, bond_state, connectivity, global_state = inputs\n",
    "            \n",
    "        batch_size = tf.shape(atom_state)[0]\n",
    "\n",
    "        graph_elements = tf.concat([atom_state, bond_state], axis=1)\n",
    "        query = self.query_layer(graph_elements)  # [B,N,S,H]\n",
    "        query = tf.transpose(query, perm=[0, 2, 1])\n",
    "        value = self.transpose_scores(self.value_layer(graph_elements))  # [B,N,S,H]\n",
    "        \n",
    "        attention_probs = tf.nn.softmax(query)\n",
    "        context = tf.matmul(tf.expand_dims(attention_probs, 2), value)        \n",
    "        context = tf.reshape(context, [batch_size, self.num_heads*self.units])        \n",
    "        \n",
    "        if self.use_global:\n",
    "            global_state = self.add([global_state, context])\n",
    "        else:\n",
    "            global_state = context\n",
    "            \n",
    "        return global_state\n",
    "    \n",
    "\n",
    "class GraphBlock(GraphLayer):\n",
    "    def __init__(self, units, num_heads, **kwargs):\n",
    "        super(GraphBlock, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(GraphBlock, self).build(input_shape)\n",
    "        self.layer_norm1 = layers.LayerNormalization()\n",
    "        self.layer_norm2 = layers.LayerNormalization()\n",
    "        \n",
    "        if self.use_global:\n",
    "            self.layer_norm3 = layers.LayerNormalization()\n",
    "        \n",
    "        self.edge_layer = EdgeUpdate()\n",
    "        self.node_layer = NodeUpdate()\n",
    "        self.global_layer = GlobalUpdate(self.units, self.num_heads)\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        if not self.use_global:\n",
    "            atom_state, bond_state, connectivity = inputs\n",
    "            atom_state = self.layer_norm1(atom_state)\n",
    "            bond_state = self.layer_norm2(bond_state)\n",
    "            \n",
    "            bond_state = self.edge_layer([atom_state, bond_state, connectivity])\n",
    "            atom_state = self.node_layer([atom_state, bond_state, connectivity])\n",
    "            global_state = self.global_layer([atom_state, bond_state, connectivity])            \n",
    "            \n",
    "        else:\n",
    "            atom_state, bond_state, connectivity, global_state = inputs\n",
    "            atom_state = self.layer_norm1(atom_state)\n",
    "            bond_state = self.layer_norm2(bond_state)            \n",
    "            global_state = self.layer_norm3(global_state)\n",
    "            \n",
    "            bond_state = self.edge_layer([atom_state, bond_state, connectivity, global_state])\n",
    "            atom_state = self.node_layer([atom_state, bond_state, connectivity, global_state])\n",
    "            global_state = self.global_layer([atom_state, bond_state, connectivity, global_state])\n",
    "            \n",
    "        return atom_state, bond_state, global_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "atom (InputLayer)               [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bond (InputLayer)               [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "connectivity (InputLayer)       [(None, None, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "n_atom (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atom_embedding_model (Model)    (None, None, 128)    2249079     atom[0][0]                       \n",
      "                                                                 bond[0][0]                       \n",
      "                                                                 connectivity[0][0]               \n",
      "                                                                 n_atom[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1)      129         atom_embedding_model[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "atom_mean (Embedding)           (None, None, 1)      204         atom[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 1)      0           dense[0][0]                      \n",
      "                                                                 atom_mean[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "atom_inf_mask (AtomInfMask)     None                 0           add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 2,249,412\n",
      "Trainable params: 2,249,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "atom_features = 128\n",
    "num_messages = 6\n",
    "\n",
    "# Define keras model\n",
    "n_atom = layers.Input(shape=[], dtype=tf.int64, name='n_atom')\n",
    "atom_class = layers.Input(shape=[None], dtype=tf.int64, name='atom')\n",
    "bond_class = layers.Input(shape=[None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, 2], dtype=tf.int64, name='connectivity')\n",
    "\n",
    "input_tensors = [atom_class, bond_class, connectivity, n_atom]\n",
    "\n",
    "# Initialize the atom states\n",
    "atom_state = layers.Embedding(preprocessor.atom_classes, atom_features,\n",
    "                              name='atom_embedding', mask_zero=True)(atom_class)\n",
    "\n",
    "# Initialize the bond states\n",
    "bond_state = layers.Embedding(preprocessor.bond_classes, atom_features,\n",
    "                              name='bond_embedding', mask_zero=True)(bond_class)\n",
    "\n",
    "\n",
    "global_state = layers.Embedding(preprocessor.max_atoms, 1,\n",
    "                                name='global_state')(n_atom)\n",
    "\n",
    "def message_block(atom_state, bond_state, connectivity, global_state):\n",
    "\n",
    "    atom_state = layers.LayerNormalization()(atom_state)\n",
    "    bond_state = layers.LayerNormalization()(bond_state)\n",
    "    global_state = layers.LayerNormalization()(global_state)\n",
    "    \n",
    "    bond_state = EdgeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
    "    atom_state = NodeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
    "    global_state = GlobalUpdate(16, 8)([atom_state, bond_state, connectivity, global_state])\n",
    "    \n",
    "    return atom_state, bond_state, global_state\n",
    "\n",
    "for i in range(num_messages):\n",
    "    atom_state, bond_state, global_state = message_block(atom_state, bond_state, connectivity, global_state)\n",
    "    \n",
    "atom_embedding_model = tf.keras.Model(input_tensors, atom_state, name='atom_embedding_model')\n",
    "\n",
    "\n",
    "\n",
    "n_atom = layers.Input(shape=[], dtype=tf.int64, name='n_atom')\n",
    "atom_class = layers.Input(shape=[None], dtype=tf.int64, name='atom')\n",
    "bond_class = layers.Input(shape=[None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, 2], dtype=tf.int64, name='connectivity')\n",
    "\n",
    "input_tensors = [atom_class, bond_class, connectivity, n_atom]\n",
    "\n",
    "atom_state = atom_embedding_model(input_tensors)\n",
    "\n",
    "atom_mean = layers.Embedding(preprocessor.atom_classes, 1,\n",
    "                             name='atom_mean', mask_zero=True)(atom_class)\n",
    "\n",
    "atom_pred = layers.Dense(1)(atom_state)\n",
    "atom_pred = layers.Add()([atom_pred, atom_mean])\n",
    "atom_pred = AtomInfMask()(atom_pred)\n",
    "\n",
    "model = tf.keras.Model(input_tensors, atom_pred)\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.InverseTimeDecay(1E-4, 1, 1E-5)\n",
    "model.compile(loss=KLWithLogits(), optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3192\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.1270\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.1031\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.0946\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.0861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f98357d04d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,\n",
    "          steps_per_epoch=100,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
