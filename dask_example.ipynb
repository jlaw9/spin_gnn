{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p debug\n",
      "#SBATCH -A rlmolecule\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=36\n",
      "#SBATCH --mem=90000\n",
      "#SBATCH -t 30\n",
      "\n",
      "/projects/rlmolecule/pstjohn/envs/tf2_gpu/bin/python -m distributed.cli.dask_worker tcp://10.148.8.97:33499 --nthreads 1 --nprocs 18 --memory-limit 5.00GB --name name --nanny --death-timeout 60 --local-directory /tmp/scratch/dask-worker-space --interface ib0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/rlmolecule/pstjohn/envs/tf2_gpu/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37203 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "# PSJ: Largely following Noel's script here for the configuration\n",
    "\n",
    "n_processes = 18  # number of processes to run on each node\n",
    "memory = 90000  # to fit on a standard node; ask for 184,000 for a bigmem node\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    project='rlmolecule',\n",
    "    walltime='30',  # 30 minutes to fit in the debug queue; 180 to fit in short\n",
    "    job_mem=str(memory),\n",
    "    job_cpu=36,\n",
    "    interface='ib0',\n",
    "    local_directory='/tmp/scratch/dask-worker-space',\n",
    "    cores=18,\n",
    "    processes=n_processes,\n",
    "    memory='{}MB'.format(memory),\n",
    "    queue='debug'  # Obviously this is limited to only a single job -- comment this out for larger runs\n",
    ")\n",
    "\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client\n",
    "dask_client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 1 # set this to the number of nodes you would like to start as workers\n",
    "cluster.scale(n_processes * n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           3937554       gpu crystal_  pstjohn  R 1-10:10:15      1 r103u01\n",
      "           3953949     debug dask-wor  pstjohn  R      18:47      1 r3i7n35\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbparams = {\n",
    "    'dbname': 'bde',\n",
    "    'port': 5432,\n",
    "    'host': 'yuma.hpc.nrel.gov',\n",
    "    'user': 'bdeops',\n",
    "    'password': '*****!',  # deleting the password from the repo\n",
    "    'options': f'-c search_path=bde',\n",
    "}\n",
    "\n",
    "with psycopg2.connect(**dbparams) as conn:\n",
    "\n",
    "    redf = pd.read_sql_query(\"\"\"\n",
    "    SELECT * from redoxcompound where estate='radical' and status='finished' limit 500\n",
    "    \"\"\", conn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_calc(logfile, atom_index):\n",
    "    \"\"\" Write your expensive calculation here \"\"\"\n",
    "    time.sleep(1)\n",
    "    return np.random.random()\n",
    "\n",
    "def apply_func(row):\n",
    "    mol = rdkit.Chem.MolFromSmiles(row.smiles)\n",
    "    results = np.asarray([(atom.GetIdx(), do_calc(row.logfile, atom.GetIdx())) for atom in mol.GetAtoms()])\n",
    "    df = pd.DataFrame(results, columns=['atom_index', 'buried_vol'])\n",
    "    df['atom_index'] = df['atom_index'].astype(int)\n",
    "    df['smiles'] = row.smiles\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_index</th>\n",
       "      <th>buried_vol</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.878320</td>\n",
       "      <td>[CH2]C1=COCC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.252866</td>\n",
       "      <td>[CH2]C1=COCC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.167195</td>\n",
       "      <td>[CH2]C1=COCC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.977913</td>\n",
       "      <td>[CH2]C1=COCC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.362401</td>\n",
       "      <td>[CH2]C1=COCC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.923857</td>\n",
       "      <td>[CH2]C1=COCC1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   atom_index  buried_vol         smiles\n",
       "0           0    0.878320  [CH2]C1=COCC1\n",
       "1           1    0.252866  [CH2]C1=COCC1\n",
       "2           2    0.167195  [CH2]C1=COCC1\n",
       "3           3    0.977913  [CH2]C1=COCC1\n",
       "4           4    0.362401  [CH2]C1=COCC1\n",
       "5           5    0.923857  [CH2]C1=COCC1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(redf.head(1).apply(apply_func, 1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "redf_dask = dd.from_pandas(redf, npartitions=10*n_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame([[1, .1, 'test']], columns=['atom_index', 'buried_vol', 'smiles'])\n",
    "results = redf_dask.map_partitions(lambda x: pd.concat(x.apply(apply_func, 1).values), meta=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the big computation. you can monitor it by going to http://localhost:1235/proxy/37203/status (replace 1235 with whatever your local jupyterlab tunnel is routed to, and 37203 with the port you get in the second cell). More info here: https://jobqueue.dask.org/en/latest/interactive.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = results.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly even ~8 millions floats isn't a ton of data. There's no real need for the SQL database if we're not using it's transactional capabilities, so we can just save this to a zipped CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished.to_csv('buried_volumes.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
